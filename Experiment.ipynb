{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Cross Lingual Transfer for Intent Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to prepare the data from Schuster et al. For now we are only examining English and Spanish datasets, since preprocessing Thai requires extra steps and is slightly more complex(tokenization). Firstly, we parse the tsv data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import pickle\n",
    "import sklearn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'label_map.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d6f0265ac2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_map.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'label_map.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "mapping = {}\n",
    "with open('label_map.json','r') as f:\n",
    "    mapping = json.load(f)\n",
    "    mapping = {int(k):v for k,v in mapping.items()}\n",
    "    \n",
    "    \n",
    "# preprocess training and test files to pandas df\n",
    "\n",
    "# eng train\n",
    "en_df, en_mapping = df_format((\"/home/santi/BA/multilingual_task_oriented_dialog_slotfilling/en/train-en.tsv\"),mapping)\n",
    "en_df.to_pickle(\"training_files/en_train.p\")\n",
    "\n",
    "# eng eval\n",
    "en_df_eval, en_mapping = df_format(\"/home/santi/BA/multilingual_task_oriented_dialog_slotfilling/en/eval-en.tsv\",mapping)\n",
    "en_df_eval.to_pickle(\"training_files/en_eval.p\")\n",
    "\n",
    "# eng test\n",
    "en_df_test, en_mapping = df_format(\"/home/santi/BA/multilingual_task_oriented_dialog_slotfilling/en/test-en.tsv\",mapping)\n",
    "en_df_test.to_pickle(\"training_files/en_test.p\")\n",
    "\n",
    "# es train\n",
    "es_df, es_mapping = df_format(\"/home/santi/BA/multilingual_task_oriented_dialog_slotfilling/es/train-es.tsv\",mapping)\n",
    "es_df.to_pickle(\"training_files/es_train.p\")\n",
    "\n",
    "# es eval\n",
    "es_df_eval, es_mapping = df_format(\"/home/santi/BA/multilingual_task_oriented_dialog_slotfilling/es/eval-es.tsv\",mapping)\n",
    "es_df_eval.to_pickle(\"training_files/es_eval.p\")\n",
    "\n",
    "# es test\n",
    "es_df_eval, es_mapping = df_format(\"/home/santi/BA/multilingual_task_oriented_dialog_slotfilling/es/test-es.tsv\",mapping)\n",
    "es_df_eval.to_pickle(\"training_files/es_test.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we treat the different subcategories as different tags. It maybe possible to employ a multi-labeled or layered setup.\n",
    "The labels are mapped to a unique integer for ease of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the dataset contains a significant amount of duplicates. We remove all instances of duplicates and take a look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = en_df.drop_duplicates(\"text\")\n",
    "es_train = es_df.drop_duplicates(\"text\")\n",
    "en_eval = en_df_eval.drop_duplicates(\"text\")\n",
    "es_eval = es_df_eval.drop_duplicates(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data for english is magnitudes of order larger than the spanish dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sent_l(df):\n",
    "    return sum([len(l.split()) for l in df[\"text\"]])/len(df)\n",
    "\n",
    "def lexical_diversity(df):\n",
    "    lexes = set()\n",
    "    for l in df[\"text\"]:\n",
    "        for w in l.split():\n",
    "            lexes.add(w)\n",
    "    return len(lexes), lexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average sentence length\")\n",
    "print(avg_sent_l(en_train))\n",
    "print(avg_sent_l(es_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unique tokens\")\n",
    "print(lexical_diversity(en_train)[0])\n",
    "print(lexical_diversity(es_train)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intersection of train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utterances in eval are mutually exclusive between train and eval for english and (mostly) for spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unique utterances en\")\n",
    "unique_sents = []\n",
    "for sent in en_eval[\"text\"]:\n",
    "    if sent not in en_train[\"text\"]:\n",
    "        unique_sents.append(sent)\n",
    "print(len(unique_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sents = []\n",
    "for sent in es_eval[\"text\"]:\n",
    "    if sent not in es_train[\"text\"]:\n",
    "        unique_sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unique utterances es\")\n",
    "print(len(unique_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However there is no variation in vocabulary whatsoever. #TODO show this again\n",
    "We now move on to the actual experiments. We begin by establishing a few baselines for the performances.\\\n",
    "\n",
    "0. Bert train En test En\n",
    "1. XLM/XLMR Intent Detection (sequence classification)\n",
    "    1. train 0 test En\n",
    "    2. train 0 test Es\n",
    "    3. train En test En\n",
    "    4. train Es test Es\n",
    "    5. train En test es\n",
    "    6. train En + Es test Es\n",
    "2. XLMR (Token Classification) Slotfilling\n",
    "    1. train 0 test En\n",
    "    2. train 0 test Es\n",
    "    3. train En test En\n",
    "    4. train Es test Es\n",
    "    5. train En test es\n",
    "    6. train En + Es test Es\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load up a pretrained XLM model with a Max Ent layer for classification. Arguments are left pretty vanilla except fp16 which is not relevant for the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "macro = lambda x,y:  sklearn.metrics.f1_score(x,y, average= 'macro')\n",
    "micro = lambda x,y:  sklearn.metrics.f1_score(x,y, average= 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\"fp16\": False,'learning_rate':1e-5, 'num_train_epochs': 2, 'reprocess_input_data': True, 'overwrite_output_dir': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint-14370-epoch-5# equal to from_pretrained in huggingface library\n",
    "model_en= ClassificationModel('xlm','xlm-mlm-xnli15-1024', num_labels=12, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, a, b = model_en.eval_model(en_eval, macro=macro, micro=micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in b:\n",
    "    print(t.text_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the results from the un fine-tuned models are abysmal. Now we examine the results with a model I finetuned on the whole english training dataset for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en.train_model(en_train,output_dir = \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, a, b = model_en.eval_model(en_eval, macro=macro, micro=micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for reloading\n",
    "#models/checkpoint-14370-epoch-5 for en only\n",
    "#model = ClassificationModel('xlm','models/checkpoint-14370-epoch-5', num_labels=12, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results, a, b = model.eval_model(en_eval, macro=macro, micro=micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res,a,b= model.eval_model(es_eval, macro=macro, micro=micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(es_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_eval.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(b))\n",
    "dom_corr = 0\n",
    "for  t in b:\n",
    "    \n",
    "    lab_pred = mapping[np.argmax(a[t.guid])]\n",
    "    lab_true = mapping[t.label]\n",
    "    dom_pred = lab_pred.split(\"/\")[0]\n",
    "    dom_true = lab_true.split(\"/\")[0]\n",
    "    if dom_pred == dom_true:\n",
    "        dom_corr += 1\n",
    "    print(t.guid)\n",
    "    print(t.text_a,\"\\t\" ,lab_pred,\"\\t\", lab_true,\"\\t\", dom_pred,\"\\t\", dom_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongs = [(inp.text_a,inp.label) for inp in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds , vecs = model.predict([t for t,l in wrongs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_corr = 0\n",
    "weak_dom = 0\n",
    "rem_alarms = [\"reminder\",\"alarm\"]\n",
    "\n",
    "for (text, lab_true), lab_pred in zip(wrongs,wrong_preds):\n",
    "    \n",
    "    lab_pred = mapping[lab_pred]\n",
    "    lab_true = mapping[lab_true]\n",
    "    dom_pred = lab_pred.split(\"/\")[0]\n",
    "    dom_true = lab_true.split(\"/\")[0]\n",
    "    \n",
    "    if dom_pred == dom_true:\n",
    "        dom_corr += 1\n",
    "        \n",
    "    if (dom_pred in rem_alarms) and (dom_true in rem_alarms):\n",
    "        weak_dom += 1    \n",
    "        \n",
    "        \n",
    "    print(text,\"\\t\" ,lab_pred,\"\\t\", lab_true,\"\\t\", dom_pred,\"\\t\", dom_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dom_corr/len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weak_dom/len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([\"ตั้ง นาฬิกา ปลุก\", \"set an alarm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, arr = model.predict([\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(es_train,output_dir = \"spanish/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es= ClassificationModel('xlm','xlm-mlm-xnli15-1024', num_labels=12, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es.train_model(es_train,output_dir = \"spanish/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es.eval_model(es_eval, macro=macro, micro=micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es.train_model(en_train,output_dir = \"spanish/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
