{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Cross Lingual Transfer for Intent Detection\n",
    "## Santichai Pornavalai\n",
    "## 10.2.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to run the experiments for Intent Only cross-lingual experiments using XLM-R. The blocks for training and testing are meant to be run individually and correspond to the experiments listed in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff9da26c150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing.util import *\n",
    "import pickle\n",
    "import sklearn\n",
    "import torch\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "torch.manual_seed(136)\n",
    "#from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening data/en/train-en.tsv\n",
      "opening data/en/eval-en.tsv\n",
      "opening data/en/test-en.tsv\n",
      "opening data/es/train-es.tsv\n",
      "opening data/es/eval-es.tsv\n",
      "opening data/es/test-es.tsv\n",
      "opening data/th/train-th_TH.tsv\n",
      "opening data/th/eval-th_TH.tsv\n",
      "opening data/th/test-th_TH.tsv\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "with open('preprocessing/label_map.json','r') as f:\n",
    "    mapping = json.load(f)\n",
    "    mapping = {int(k):v for k,v in mapping.items()}\n",
    "    \n",
    "    \n",
    "# preprocess training and test files to pandas df\n",
    "\n",
    "# eng train\n",
    "en_df, en_mapping = df_format((\"data/en/train-en.tsv\"),mapping)\n",
    "\n",
    "# eng eval\n",
    "en_df_eval, en_mapping = df_format(\"data/en/eval-en.tsv\",mapping)\n",
    "\n",
    "# eng test\n",
    "en_df_test, en_mapping = df_format(\"data/en/test-en.tsv\",mapping)\n",
    "\n",
    "# es train\n",
    "es_df, es_mapping = df_format(\"data/es/train-es.tsv\",mapping)\n",
    "\n",
    "# es eval\n",
    "es_df_eval, es_mapping = df_format(\"data/es/eval-es.tsv\",mapping)\n",
    "\n",
    "# es test\n",
    "es_df_test, es_mapping = df_format(\"data/es/test-es.tsv\",mapping)\n",
    "\n",
    "\n",
    "# th train\n",
    "th_df, th_mapping = df_format(\"data/th/train-th_TH.tsv\",mapping)\n",
    "\n",
    "# th eval\n",
    "th_df_eval, th_mapping = df_format(\"data/th/eval-th_TH.tsv\",mapping)\n",
    "\n",
    "# th test\n",
    "th_df_test, th_mapping = df_format(\"data/th/test-th_TH.tsv\",mapping)\n",
    "\n",
    "mapping_list = list(mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop some duplicate values \n",
    "# This is perhaps unnecessary. \n",
    "en_train = en_df.drop_duplicates(\"text\")\n",
    "en_eval = en_df_eval.drop_duplicates(\"text\")\n",
    "en_test = en_df_test.drop_duplicates(\"text\")\n",
    "\n",
    "es_train = es_df.drop_duplicates(\"text\")\n",
    "es_eval = es_df_eval.drop_duplicates(\"text\")\n",
    "es_test = es_df_test.drop_duplicates(\"text\")\n",
    "\n",
    "th_train = th_df.drop_duplicates(\"text\")\n",
    "th_eval = th_df_eval.drop_duplicates(\"text\")\n",
    "th_test = th_df_test.drop_duplicates(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not know if the other experiments were trained on a combination of train and eval, we err on the safe side ignore the eval file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_full_train = pd.concat([en_train,en_eval])\n",
    "# es_full_train = pd.concat([es_train,es_eval])\n",
    "# th_full_train = pd.concat([th_train, th_eval])\n",
    "# quick hack to by-pass combining en eval \n",
    "\n",
    "en_full_train = en_train\n",
    "es_full_train = es_train\n",
    "th_full_train = th_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we no longer use these paths.\n",
    "# should be removed\n",
    "path2model = \"prelim_models/\"\n",
    "path2model_en = \"prelim_models/en/\"\n",
    "path2model_es = \"prelim_models/es/\"\n",
    "path2model_th = \"prelim_models/th/\"\n",
    "#path2model_x = \"/home/santi/BA/final_models/x/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix create data for cross-lingual training\n",
    "en_th_full_train = pd.concat([en_full_train,th_full_train])\n",
    "en_es_full_train = pd.concat([en_full_train,th_full_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_sent_l(df):\n",
    "    return sum([len(l.split()) for l in df[\"text\"]])/len(df)\n",
    "\n",
    "def lexical_diversity(df):\n",
    "    lexes = set()\n",
    "    for l in df[\"text\"]:\n",
    "        for w in l.split():\n",
    "            lexes.add(w)\n",
    "    return len(lexes), lexes\n",
    "\n",
    "\n",
    "def analyze_wrong(wrong_predictions,model):\n",
    "    wrongs = [(inp.text_a,inp.label) for inp in wrong_predictions]\n",
    "    wrong_preds, vecs = model.predict([t for t,l in wrongs])\n",
    "\n",
    "    dom_corr = 0\n",
    "    weak_dom = 0\n",
    "    rem_alarms = [\"reminder\",\"alarm\"]\n",
    "    results = []\n",
    "\n",
    "    for (text, lab_true), lab_pred in zip(wrongs,wrong_preds):\n",
    "\n",
    "        lab_pred = mapping[lab_pred]\n",
    "        lab_true = mapping[lab_true]\n",
    "        dom_pred = lab_pred.split(\"/\")[0]\n",
    "        dom_true = lab_true.split(\"/\")[0]\n",
    "\n",
    "        if dom_pred == dom_true:\n",
    "            dom_corr += 1\n",
    "\n",
    "        if (dom_pred in rem_alarms) and (dom_true in rem_alarms):\n",
    "            weak_dom += 1    \n",
    "\n",
    "        results.append((text,lab_pred, lab_true))\n",
    "\n",
    "        #print(text,\"\\t\" ,lab_pred,\"\\t\", lab_true,\"\\t\", dom_pred,\"\\t\", dom_true)\n",
    "\n",
    "    return results, dom_corr/len(wrongs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics used for evaluation\n",
    "macro = lambda x,y:  sklearn.metrics.f1_score(x,y, average= 'macro')\n",
    "micro = lambda x,y:  sklearn.metrics.f1_score(x,y, average= 'micro')\n",
    "report = lambda x,y:  sklearn.metrics.classification_report(x,y,digits = 5,labels = list(range(0,12)), target_names = mapping_list)\n",
    "report_dict = lambda x,y:  sklearn.metrics.classification_report(x,y,digits = 5,output_dict = True,labels = list(range(0,12)),target_names = mapping_list)\n",
    "accuracy = lambda x,y:  sklearn.metrics.accuracy_score(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval(df, model, ex_name = \"experiment 1\", verbose = True):\n",
    "    results, predictions_vs, wrongs = model.eval_model(df, macro=macro, micro=micro,accuracy=accuracy, report=report, report_dict = report_dict)\n",
    "    results[\"name\"] = ex_name\n",
    "    \n",
    "    false_preds,dom_acc = analyze_wrong(wrongs,model)\n",
    "    results[\"wrong_predictions\"] = false_preds\n",
    "    results[\"domain_of_wrongs\"] = dom_acc\n",
    "    results[\"domain_accuracy\"] = results[\"accuracy\"] + (1-results[\"accuracy\"])*dom_acc\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"results for experiment: \",ex_name)\n",
    "\n",
    "        print(results[\"report\"])\n",
    "        print(\"domain accuracy: \",results[\"domain_accuracy\"])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gather some statistics about the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sentence length\n",
      "en 7.69126027754818\n",
      "es 7.678731678133413\n",
      "th 8.280373831775702\n",
      "unique tokens\n",
      "en 3983\n",
      "es 1849\n",
      "th 1138\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "print(\"average sentence length\")\n",
    "print(\"en\",avg_sent_l(en_train))\n",
    "print(\"es\",avg_sent_l(es_train))\n",
    "print(\"th\",avg_sent_l(th_train))\n",
    "\n",
    "\n",
    "print(\"unique tokens\")\n",
    "print(\"en\",lexical_diversity(en_train)[0])\n",
    "print(\"es\",lexical_diversity(es_train)[0])\n",
    "print(\"th\",lexical_diversity(th_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load up a pretrained XLM-R model with a Max Ent layer for classification. Arguments are left pretty vanilla except fp16 which is not relevant for the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the hyper-parameters here. \n",
    "\n",
    "args={\"fp16\": True,\n",
    "      'learning_rate':1e-5,\n",
    "      'num_train_epochs': 5,\n",
    "      'reprocess_input_data': True,\n",
    "      'overwrite_output_dir': True,\n",
    "      'save_steps':-1,\n",
    "      \"save_model_every_epoch\":False,\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7ff9b8c51c20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/santi/anaconda3/lib/python3.7/site-packages/tqdm/std.py\", line 1085, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b17f261fb04af49922d5085363118a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22987.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaeb66d3cac4d71baf8060c035a103a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b370bbb89aa44335959619fdb9108cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 5', max=2874.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-21e221f682a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_dir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"models/intent_en_train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xlmroberta'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'xlm-roberta-base'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_full_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# test eng\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0meval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         )\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m                         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train english model\n",
    "# full train = train + eval\n",
    "args[\"output_dir\"] = \"models/intent_en_train\"\n",
    "model= ClassificationModel('xlmroberta','xlm-roberta-base', num_labels=12, args=args)\n",
    "model.train_model(en_full_train)\n",
    "\n",
    "# test eng\n",
    "results = custom_eval(en_test, model, \"train_en_test_en\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "# test es\n",
    "results = custom_eval(es_test, model, \"train_en_test_es\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "# test th\n",
    "results = custom_eval(th_test, model, \"train_en_test_th\")\n",
    "experiment_results[results[\"name\"]] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset model\n",
    "# train on full spanish\n",
    "args[\"output_dir\"] = \"models/intent_es_train\"\n",
    "\n",
    "model= ClassificationModel('xlmroberta','xlm-roberta-base', num_labels=12, args=args)\n",
    "model.train_model(es_full_train)\n",
    "\n",
    "# test eng\n",
    "results = custom_eval(en_test, model, \"train_es_test_en\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test es\n",
    "results = custom_eval(es_test, model, \"train_es_test_es\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test th\n",
    "results = custom_eval(th_test, model, \"train_es_test_th\")\n",
    "experiment_results[results[\"name\"]] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset model\n",
    "args[\"output_dir\"] = \"models/intent_th_train\"\n",
    "\n",
    "model= ClassificationModel('xlmroberta','xlm-roberta-base', num_labels=12, args=args)\n",
    "# train on full thai\n",
    "model.train_model(th_full_train)\n",
    "\n",
    "# test eng\n",
    "results = custom_eval(en_test, model, \"train_th_test_en\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test es\n",
    "results = custom_eval(es_test, model, \"train_th_test_es\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test th\n",
    "results = custom_eval(th_test, model, \"train_th_test_th\")\n",
    "experiment_results[results[\"name\"]] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"output_dir\"] = \"models/intent_en_th_train\"\n",
    "\n",
    "model= ClassificationModel('xlmroberta','xlm-roberta-base', num_labels=12, args=args)\n",
    "# train on full thai and eng mixed\n",
    "model.train_model(en_th_full_train)\n",
    "# test eng\n",
    "results = custom_eval(en_test, model, \"train_th_test_en\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test es\n",
    "results = custom_eval(es_test, model, \"train_th_test_es\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test th\n",
    "results = custom_eval(th_test, model, \"train_th_test_th\")\n",
    "experiment_results[results[\"name\"]] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"output_dir\"] = \"models/intent_en_es_train\"\n",
    "\n",
    "model= ClassificationModel('xlmroberta','xlm-roberta-base', num_labels=12, args=args)\n",
    "# train on full thai and eng mixed\n",
    "model.train_model(en_es_full_train)\n",
    "# test eng\n",
    "results = custom_eval(en_test, model, \"train_th_test_en\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test es\n",
    "results = custom_eval(es_test, model, \"train_th_test_es\")\n",
    "experiment_results[results[\"name\"]] = results\n",
    "\n",
    "\n",
    "# test th\n",
    "results = custom_eval(th_test, model, \"train_th_test_th\")\n",
    "experiment_results[results[\"name\"]] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some sanity checks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, predicted, real in experiment_results[\"train_en_test_th\"][\"wrong_predictions\"]:\n",
    "    print(text, \"\\t\", predicted, \"\\t\", real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SANITY CHECK #####\n",
    "def unique_sents(test_df, train_df):\n",
    "    print(\"unique utterances in test data out of :\", len(test_df))\n",
    "    unique_sents = []\n",
    "    train_set = set(train_df[\"text\"])\n",
    "    for sent in test_df[\"text\"]:\n",
    "        if sent not in train_set:\n",
    "            unique_sents.append(sent)\n",
    "    print(len(unique_sents)/len(test_df)*100,\"% of the sentences are unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sents(en_test,en_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sents(es_test, es_eval)\n",
    "unique_sents(es_test,es_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sents(th_test, th_eval)\n",
    "unique_sents(th_test,th_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent = lambda sent: mapping[model.predict([sent])[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"what's the weather in Potsdam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"don't wake me up tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict_sent(\"ตั้ง เวลา พรุ่ง บ่าย พรุ่งนี้\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"que temperatura hay aqui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"no necesito que levantarme el sabado\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"sabado no necesito que levantarme\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"ไม่ ต้อง ปลุก ฉัน วัน เสาร์ นะ\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_sent(\"วัน เสาร์ ไม่ ต้อง ปลุก ฉัน นะ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"you don't have to wake me up on saturday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"saturday you don't have to wake me up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_full_train[en_full_train[\"text\"].str.contains(\"^on (saturday|sunday|monday|tuesday)\",case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_sent(\"I don't have to wake up early on saturday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a weird sentence \n",
    "predict_sent(\"saturday you don't have to wake me up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"am Samstag musst du mich nicht aufwecken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sent(\"ich nicht muss aufstehen am Samstag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"el sabado no necesito el despertador\" \n",
    "# doesn't work\n",
    "# implicit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"cuanto falta hasta el alarma\"\n",
    "\"cuanto tiempo queda hasta que me levanto\"\n",
    "\"que temperatura hay aqui\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
